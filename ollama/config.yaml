name: "Ollama"
description: "The easiest way to get up and running with large language models locally"
slug: "ollama"
init: false
arch:
  - aarch64
  - amd64
url: https://ollama.com
ports:
  11434/tcp: 11434
image: docker.io/ollama/ollama
version: "0.1.33"
ingress: true
ingress_port: 11434
ingress_stream: true
map:
  - config:rw
environment:
  OLLAMA_MODELS: "/config/ollama"
  OLLAMA_NUM_PARALLEL: "4"
  OLLAMA_MAX_LOADED_MODELS: "4"
